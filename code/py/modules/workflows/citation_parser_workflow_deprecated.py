"""
å¼•ç”¨æ–‡çŒ®ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

æ§˜ã€…ãªå½¢å¼ã®å¼•ç”¨æ–‡çŒ®ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã€
ãƒªãƒ³ã‚¯ä»˜ãå¼•ç”¨ã‹ã‚‰ã®å¯¾å¿œè¡¨ç”Ÿæˆã‚’è¡Œã†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import time
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import click

from ..shared.exceptions import (
    CitationParserError, InvalidCitationPatternError, CitationParseTimeoutError
)
from ..citation_parser.citation_parser import CitationParser


class CitationParserWorkflow:
    """å¼•ç”¨æ–‡çŒ®ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
    
    def __init__(self, config_manager, logger):
        """
        Args:
            config_manager: è¨­å®šç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            logger: çµ±åˆãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.config_manager = config_manager
        self.logger = logger.get_logger('Workflows.CitationParser')
        self.config = config_manager.get_citation_parser_config()
        
        # Citation Parserã®åˆæœŸåŒ–
        # è¨­å®šè¾æ›¸ã‹ã‚‰ä¸€æ™‚çš„ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹ã€ç›´æ¥åˆæœŸåŒ–ã™ã‚‹
        self.parser = CitationParser()
        
    def execute(self, input_file: str = None, clippings_dir: str = None, **options) -> Tuple[bool, Dict[str, Any]]:
        """
        å¼•ç”¨æ–‡çŒ®ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        
        Args:
            input_file: å…¥åŠ›Markdownãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç”¨ï¼‰
            clippings_dir: Clippingsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ç”¨ï¼‰
            **options: å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
                - output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯æ¨™æº–å‡ºåŠ›ï¼‰
                - pattern_type: ãƒ‘ãƒ¼ã‚¹å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆbasic|advanced|allï¼‰
                - output_format: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆunified|table|jsonï¼‰
                - enable_link_extraction: ãƒªãƒ³ã‚¯æŠ½å‡ºæœ‰åŠ¹åŒ–
                - expand_ranges: ç¯„å›²å¼•ç”¨å±•é–‹
                - dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
                - verbose: è©³ç´°å‡ºåŠ›
        
        Returns:
            (æˆåŠŸãƒ•ãƒ©ã‚°, å®Ÿè¡Œçµæœè©³ç´°)
        """
        start_time = time.time()
        
        try:
            # å…¥åŠ›ã®æ¤œè¨¼ã¨ãƒ¢ãƒ¼ãƒ‰æ±ºå®š
            if clippings_dir:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                return self._execute_directory_mode(clippings_dir, options, start_time)
            elif input_file:
                # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                return self._execute_single_file_mode(input_file, options, start_time)
            else:
                raise ValueError("Either input_file or clippings_dir must be specified")
                
        except Exception as e:
            self.logger.error(f"Citation parser workflow failed: {e}")
            return False, {"error": str(e), "processing_time": time.time() - start_time}
    
    def _execute_single_file_mode(self, input_file: str, options: Dict, start_time: float) -> Tuple[bool, Dict[str, Any]]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰"""
        self.logger.info("Starting citation parser workflow")
        self.logger.info(f"Input file: {input_file}")
        
        # å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒãƒ¼ã‚¸
        execution_options = {**self.config, **options}
        
        # è¨­å®šã‚’ä¸€æ™‚çš„ã«æ›´æ–°
        original_config = self.config.copy()
        self.config.update(execution_options)
        
        try:
            # çµæœè¾æ›¸ã®åˆæœŸåŒ–
            results = {
                "input_file": input_file,
                "execution_options": execution_options,
                "statistics": {},
                "processing_time": 0
            }
            
            # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
            self.logger.info("Validating input file")
            if not self.validate_inputs(input_file):
                raise CitationParserError(f"Input validation failed for: {input_file}")
            
            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å‡¦ç†
            if execution_options.get('dry_run', False):
                results.update(self._execute_dry_run(input_file, execution_options))
                return True, results
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            self.logger.info("Reading input file")
            with open(input_file, 'r', encoding='utf-8') as f:
                input_text = f.read()
            
            # å¼•ç”¨æ–‡çŒ®ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
            self.logger.info("Processing citations")
            parse_result = self.parser.parse_document(input_text)
            
            # çµæœã®å‡¦ç†
            results.update({
                "converted_text": parse_result.converted_text,
                "link_table": parse_result.link_table,
                "statistics": {
                    "total_citations": parse_result.statistics.total_citations,
                    "converted_citations": parse_result.statistics.converted_citations,
                    "error_count": len(parse_result.errors),
                    "processing_time": time.time() - start_time
                },
                "errors": parse_result.errors
            })
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            output_file = execution_options.get('output_file')
            if output_file:
                self.logger.info(f"Writing output to: {output_file}")
                self._write_output_file(output_file, parse_result, execution_options)
                results["output_file"] = output_file
            else:
                # æ¨™æº–å‡ºåŠ›ã¸ã®è¡¨ç¤º
                self._display_results(parse_result, execution_options)
            
            # çµ±è¨ˆæƒ…å ±ã®ãƒ­ã‚°å‡ºåŠ›
            stats = results["statistics"]
            self.logger.info(f"Processed {stats['total_citations']} citations")
            self.logger.info(f"Converted {stats['converted_citations']} citations")
            if stats['error_count'] > 0:
                self.logger.warning(f"Encountered {stats['error_count']} errors")
            
            results["processing_time"] = time.time() - start_time
            self.logger.info(f"Citation parser workflow completed in {results['processing_time']:.2f} seconds")
            
            return True, results
            
        finally:
            # è¨­å®šã‚’å¾©å…ƒ
            self.config = original_config
    
    def _execute_directory_mode(self, clippings_dir: str, options: Dict, start_time: float) -> Tuple[bool, Dict[str, Any]]:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼šClippingsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨.mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        from pathlib import Path
        import glob
        
        self.logger.info(f"Starting citation parser workflow for directory: {clippings_dir}")
        
        clippings_path = Path(clippings_dir)
        if not clippings_path.exists():
            raise ValueError(f"Clippings directory does not exist: {clippings_dir}")
        
        # .mdãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        md_files = glob.glob(str(clippings_path / "*" / "*.md"))
        
        if not md_files:
            self.logger.warning(f"No .md files found in {clippings_dir}")
            return True, {
                "success": True,
                "clippings_dir": clippings_dir,
                "processed_files": [],
                "total_files": 0,
                "processing_time": time.time() - start_time
            }
        
        self.logger.info(f"Found {len(md_files)} .md files to process in {clippings_dir}")
        
        # å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒãƒ¼ã‚¸
        execution_options = {**self.config, **options}
        
        processed_files = []
        total_citations = 0
        total_converted = 0
        total_errors = 0
        
        for md_file in md_files:
            try:
                self.logger.info(f"Processing: {md_file}")
                
                # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã®ã¿å®Ÿéš›ã«å‡¦ç†
                if not execution_options.get('dry_run', False):
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    with open(md_file, 'r', encoding='utf-8') as f:
                        input_text = f.read()
                    
                    # å¼•ç”¨æ–‡çŒ®ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
                    parse_result = self.parser.parse_document(input_text)
                    
                    # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ›´æ–°ï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼‰
                    with open(md_file, 'w', encoding='utf-8') as f:
                        f.write(parse_result.converted_text)
                    
                    # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
                    total_citations += parse_result.statistics.total_citations
                    total_converted += parse_result.statistics.converted_citations
                    total_errors += len(parse_result.errors)
                    
                    processed_files.append({
                        "file": md_file,
                        "citations_found": parse_result.statistics.total_citations,
                        "citations_converted": parse_result.statistics.converted_citations,
                        "errors": len(parse_result.errors)
                    })
                    
                    self.logger.info(f"Converted {parse_result.statistics.converted_citations}/{parse_result.statistics.total_citations} citations in {md_file}")
                else:
                    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã®å ´åˆ
                    processed_files.append({
                        "file": md_file,
                        "status": "dry_run_detected"
                    })
                
            except Exception as e:
                self.logger.error(f"Failed to process {md_file}: {e}")
                processed_files.append({
                    "file": md_file,
                    "error": str(e)
                })
                total_errors += 1
        
        # çµæœã®æ§‹æˆ
        results = {
            "success": True,
            "clippings_dir": clippings_dir,
            "processed_files": processed_files,
            "total_files": len(md_files),
            "statistics": {
                "total_citations": total_citations,
                "converted_citations": total_converted,
                "error_count": total_errors,
                "processing_time": time.time() - start_time
            },
            "processing_time": time.time() - start_time
        }
        
        self.logger.info(f"Directory processing completed: {len(processed_files)} files processed, {total_converted}/{total_citations} citations converted")
        
        return True, results
    
    def validate_inputs(self, input_file: str) -> bool:
        """
        å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        
        Args:
            input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯çµæœ
            
        Raises:
            CitationParserError: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«å•é¡ŒãŒã‚ã‚‹å ´åˆ
        """
        try:
            input_path = Path(input_file)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not input_path.exists():
                raise CitationParserError(f"Input file does not exist: {input_file}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šæ¨©é™ãƒã‚§ãƒƒã‚¯
            if not input_path.is_file():
                raise CitationParserError(f"Input path is not a file: {input_file}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            max_size_mb = self.config.get('max_file_size_mb', 10)
            file_size_mb = input_path.stat().st_size / (1024 * 1024)
            if file_size_mb > max_size_mb:
                raise CitationParserError(
                    f"File size ({file_size_mb:.1f}MB) exceeds limit ({max_size_mb}MB)"
                )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯ï¼ˆæ¨å¥¨ï¼‰
            if input_path.suffix.lower() not in ['.md', '.txt', '.markdown']:
                self.logger.warning(f"Input file extension '{input_path.suffix}' is not typical for text processing")
            
            self.logger.debug(f"Input validation passed for: {input_file}")
            return True
            
        except CitationParserError:
            raise
        except Exception as e:
            raise CitationParserError(f"Validation error: {e}")
    
    def _execute_dry_run(self, input_file: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """
        ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
        
        Args:
            input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            options: å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
            
        Returns:
            Dict[str, Any]: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³çµæœ
        """
        self.logger.info("Executing dry run - no actual changes will be made")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®å‡¦ç†ï¼‰
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç°¡æ˜“åˆ†æ
            line_count = len(content.splitlines())
            char_count = len(content)
            
            # å¼•ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç°¡æ˜“æ¤œå‡º
            import re
            patterns = [
                r'\[\d+\]',           # [1]
                r'\[\d+,\s*\d+\]',    # [1, 2]
                r'\[\d+-\d+\]',       # [1-3]
                r'\[\^\d+\]',         # [^1]
                r'\[\d+\]\([^)]+\)'   # [1](URL)
            ]
            
            detected_citations = []
            for pattern in patterns:
                matches = re.findall(pattern, content)
                detected_citations.extend(matches)
            
            dry_run_result = {
                "dry_run": True,
                "dry_run_analysis": {
                    "file_stats": {
                        "line_count": line_count,
                        "character_count": char_count,
                        "file_size_kb": len(content.encode('utf-8')) / 1024
                    },
                    "detected_citations": {
                        "count": len(detected_citations),
                        "samples": detected_citations[:10]  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
                    },
                    "processing_options": {
                        "pattern_type": options.get('pattern_type', 'all'),
                        "output_format": options.get('output_format', 'unified'),
                        "enable_link_extraction": options.get('enable_link_extraction', False),
                        "expand_ranges": options.get('expand_ranges', True)
                    }
                }
            }
            
            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³çµæœã®è¡¨ç¤º
            click.echo("\nğŸ” Citation Parser Dry Run Analysis")
            click.echo("=" * 50)
            click.echo(f"ğŸ“„ File: {input_file}")
            click.echo(f"ğŸ“Š Lines: {line_count}, Characters: {char_count}")
            click.echo(f"ğŸ“ Detected citations: {len(detected_citations)}")
            if detected_citations:
                click.echo(f"ğŸ“‹ Sample citations: {', '.join(detected_citations[:5])}")
            
            return dry_run_result
            
        except Exception as e:
            raise CitationParserError(f"Dry run failed: {e}")
    
    def _write_output_file(self, output_file: str, parse_result, options: Dict[str, Any]) -> None:
        """
        å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã¿
        
        Args:
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            parse_result: ãƒ‘ãƒ¼ã‚¹çµæœ
            options: å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
        """
        try:
            output_path = Path(output_file)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            encoding = options.get('output_encoding', 'utf-8')
            output_format = options.get('output_format', 'unified')
            
            with open(output_file, 'w', encoding=encoding) as f:
                if output_format == 'json':
                    import json
                    output_data = {
                        "converted_text": parse_result.converted_text,
                        "link_table": [
                            {"citation_number": entry.citation_number, "url": entry.url}
                            for entry in parse_result.link_table
                        ],
                        "statistics": {
                            "total_citations": parse_result.statistics.total_citations,
                            "converted_citations": parse_result.statistics.converted_citations,
                            "error_count": len(parse_result.errors)
                        }
                    }
                    json.dump(output_data, f, indent=2, ensure_ascii=False)
                
                elif output_format == 'table':
                    # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§ã®å‡ºåŠ›
                    f.write(parse_result.converted_text)
                    if parse_result.link_table:
                        f.write("\n\n## å¼•ç”¨æ–‡çŒ®ãƒªãƒ³ã‚¯å¯¾å¿œè¡¨\n\n")
                        f.write("| å¼•ç”¨ç•ªå· | URL |\n")
                        f.write("|---------|-----|\n")
                        for entry in parse_result.link_table:
                            f.write(f"| {entry.citation_number} | {entry.url} |\n")
                
                else:  # unified (default)
                    f.write(parse_result.converted_text)
                    if parse_result.link_table:
                        f.write("\n\n<!-- Citation Links -->\n")
                        for entry in parse_result.link_table:
                            f.write(f"<!-- [{entry.citation_number}]: {entry.url} -->\n")
            
            self.logger.info(f"Output written to: {output_file}")
            
        except Exception as e:
            raise CitationParserError(f"Failed to write output file: {e}")
    
    def _display_results(self, parse_result, options: Dict[str, Any]) -> None:
        """
        çµæœã‚’æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤º
        
        Args:
            parse_result: ãƒ‘ãƒ¼ã‚¹çµæœ
            options: å®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³
        """
        output_format = options.get('output_format', 'unified')
        verbose = options.get('verbose', False)
        
        if verbose:
            click.echo("\nğŸ“ Citation Parser Results")
            click.echo("=" * 50)
            click.echo(f"Total citations: {parse_result.statistics.total_citations}")
            click.echo(f"Converted citations: {parse_result.statistics.converted_citations}")
            click.echo(f"Errors: {len(parse_result.errors)}")
            click.echo("\nğŸ“„ Converted Text:")
            click.echo("-" * 30)
        
        click.echo(parse_result.converted_text)
        
        if parse_result.link_table and verbose:
            click.echo("\nğŸ”— Link Table:")
            click.echo("-" * 30)
            for entry in parse_result.link_table:
                click.echo(f"[{entry.citation_number}]: {entry.url}")
        
        if parse_result.errors and verbose:
            click.echo("\nâš ï¸  Errors:")
            click.echo("-" * 30)
            for error in parse_result.errors:
                click.echo(f"â€¢ {error}")
    
    def generate_report(self, result: Dict[str, Any]) -> str:
        """
        å®Ÿè¡Œçµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            result: å®Ÿè¡Œçµæœè¾æ›¸
            
        Returns:
            str: ãƒ¬ãƒãƒ¼ãƒˆæ–‡å­—åˆ—
        """
        if result.get('dry_run', False):
            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ
            analysis = result.get('dry_run_analysis', {})
            file_stats = analysis.get('file_stats', {})
            citations = analysis.get('detected_citations', {})
            
            report = f"""
Citation Parser Workflow - Dry Run Report
==========================================

ğŸ“„ Input File: {result.get('input_file', 'Unknown')}

ğŸ“Š File Statistics:
  â€¢ Lines: {file_stats.get('line_count', 0)}
  â€¢ Characters: {file_stats.get('character_count', 0)}
  â€¢ Size: {file_stats.get('file_size_kb', 0):.1f} KB

ğŸ“ Citation Detection:
  â€¢ Found: {citations.get('count', 0)} citations
  â€¢ Samples: {', '.join(citations.get('samples', [])[:5])}

ğŸ”§ Processing Options:
  â€¢ Pattern Type: {analysis.get('processing_options', {}).get('pattern_type', 'all')}
  â€¢ Output Format: {analysis.get('processing_options', {}).get('output_format', 'unified')}
  â€¢ Link Extraction: {analysis.get('processing_options', {}).get('enable_link_extraction', False)}
  â€¢ Range Expansion: {analysis.get('processing_options', {}).get('expand_ranges', True)}

âš ï¸  Note: This was a dry run - no actual processing was performed.
"""
        else:
            # é€šå¸¸å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ
            stats = result.get('statistics', {})
            
            report = f"""
Citation Parser Workflow Report
===============================

ğŸ“„ Input File: {result.get('input_file', 'Unknown')}
ğŸ“¤ Output File: {result.get('output_file', 'Standard Output')}

ğŸ“Š Processing Statistics:
  â€¢ Total citations found: {stats.get('total_citations', 0)}
  â€¢ Successfully converted: {stats.get('converted_citations', 0)}
  â€¢ Errors encountered: {stats.get('error_count', 0)}
  â€¢ Processing time: {stats.get('processing_time', 0):.2f} seconds

ğŸ”§ Options Used:
  â€¢ Output Format: {result.get('execution_options', {}).get('output_format', 'unified')}
  â€¢ Link Extraction: {result.get('execution_options', {}).get('enable_link_extraction', False)}
  â€¢ Range Expansion: {result.get('execution_options', {}).get('expand_ranges', True)}
"""
            
            if stats.get('error_count', 0) > 0:
                report += f"\nâš ï¸  {stats['error_count']} errors occurred during processing."
            else:
                report += "\nâœ… Processing completed successfully!"
        
        return report.strip() 