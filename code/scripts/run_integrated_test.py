"""
çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³çµŒç”±ã§çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚
Click ãƒ™ãƒ¼ã‚¹ã®CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚
"""

import click
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from code.py.modules.shared.config_manager import ConfigManager
from code.py.modules.shared.integrated_logger import IntegratedLogger
from code.integrated_test.integrated_test_runner import IntegratedTestRunner


@click.command()
@click.option('--test-type', default='full', 
              type=click.Choice(['full', 'regression', 'performance']),
              help='çµ±åˆãƒ†ã‚¹ãƒˆã®ç¨®é¡')
@click.option('--reset-environment', is_flag=True,
              help='ãƒ†ã‚¹ãƒˆç’°å¢ƒã®å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆ')
@click.option('--keep-environment', is_flag=True,
              help='ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã®ç’°å¢ƒä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰')
@click.option('--specific-modules', 
              help='ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ãƒ†ã‚¹ãƒˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
@click.option('--disable-ai-features', is_flag=True,
              help='AIæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ')
@click.option('--verbose', is_flag=True,
              help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
@click.option('--report-format', default='json',
              type=click.Choice(['json', 'html', 'text']),
              help='ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆå½¢å¼')
def run_integrated_test(test_type, reset_environment, keep_environment,
                       specific_modules, disable_ai_features, verbose, report_format):
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰"""
    
    click.echo(f"ğŸ”„ Starting integrated test (type: {test_type})")
    
    try:
        # è¨­å®šç®¡ç†ã¨ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
        config_manager = ConfigManager()
        logger = IntegratedLogger(config_manager)
        
        if verbose:
            logger.set_level("DEBUG")
        
        # çµ±åˆãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–
        test_runner = IntegratedTestRunner(config_manager, logger)
        
        # ãƒ†ã‚¹ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº–å‚™
        test_options = {
            'reset_environment': reset_environment,
            'keep_environment': keep_environment,
            'disable_ai_features': disable_ai_features,
            'verbose': verbose,
            'report_format': report_format
        }
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—åˆ¥å®Ÿè¡Œ
        if test_type == 'full':
            click.echo("ğŸ“‹ Executing full integration test...")
            result = test_runner.run_full_integration_test(test_options)
            
        elif test_type == 'regression':
            modules = []
            if specific_modules:
                modules = [m.strip() for m in specific_modules.split(',')]
            
            click.echo(f"ğŸ” Executing regression test for modules: {modules or 'all'}")
            result = test_runner.run_regression_test(modules)
            
        elif test_type == 'performance':
            click.echo("âš¡ Executing performance test...")
            result = test_runner.run_performance_test()
        
        # çµæœè¡¨ç¤º
        _display_test_result(result, report_format)
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
        if result['status'] == 'passed':
            click.echo("âœ… Integrated test completed successfully")
            sys.exit(0)
        else:
            click.echo("âŒ Integrated test failed")
            sys.exit(1)
            
    except Exception as e:
        click.echo(f"ğŸ’¥ Error during integrated test execution: {str(e)}", err=True)
        if verbose:
            import traceback
            click.echo(traceback.format_exc(), err=True)
        sys.exit(1)


def _display_test_result(result, report_format):
    """
    ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤º
    
    Args:
        result (dict): ãƒ†ã‚¹ãƒˆçµæœ
        report_format (str): ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼
    """
    click.echo("\n" + "="*60)
    click.echo("ğŸ“Š INTEGRATED TEST RESULT")
    click.echo("="*60)
    
    # åŸºæœ¬æƒ…å ±
    click.echo(f"Session ID: {result.get('test_session_id', 'unknown')}")
    click.echo(f"Status: {result.get('status', 'unknown')}")
    click.echo(f"Duration: {result.get('duration_seconds', 0):.2f} seconds")
    
    # ã‚¨ãƒ©ãƒ¼æƒ…å ±
    if 'error' in result:
        click.echo(f"âŒ Error: {result['error']}")
    
    # è©³ç´°æƒ…å ±ï¼ˆJSONå½¢å¼ï¼‰
    if report_format == 'json':
        click.echo("\nğŸ“„ Detailed Result (JSON):")
        import json
        click.echo(json.dumps(result, indent=2, ensure_ascii=False))
    
    # æ¤œè¨¼çµæœï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if 'validation_result' in result:
        validation = result['validation_result']
        click.echo(f"\nğŸ” Validation Results:")
        click.echo(f"  - YAML Headers: {'âœ…' if validation.get('yaml_headers_valid') else 'âŒ'}")
        click.echo(f"  - File Structure: {'âœ…' if validation.get('file_structure_correct') else 'âŒ'}")
        click.echo(f"  - Citation Data: {'âœ…' if validation.get('citation_data_complete') else 'âŒ'}")
        
        if validation.get('validation_errors'):
            click.echo("\nâŒ Validation Errors:")
            for error in validation['validation_errors']:
                click.echo(f"  - {error}")
    
    click.echo("="*60)


if __name__ == '__main__':
    run_integrated_test() 